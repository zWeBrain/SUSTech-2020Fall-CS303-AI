{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[rlJO, PERC, jKBF, vzCE, guFC, RCEb, RFai, sVy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[CXWx, YQuU, UKEG, VegJ, Mwlr, hDgE, rlJO, jRa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[rlJO, YcPU, VDsU, sNsm, hyTc, GyJn, Kwvw, sPn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[vzCE, Ktvi, ynUk, Qrkv, sork, guFC, hfrR, DNK...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[mdSU, jKhd, vzCE, ynUk, rlJO, pKEB, VegJ, pFZ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  value\n",
       "0  [rlJO, PERC, jKBF, vzCE, guFC, RCEb, RFai, sVy...      0\n",
       "1  [CXWx, YQuU, UKEG, VegJ, Mwlr, hDgE, rlJO, jRa...      1\n",
       "2  [rlJO, YcPU, VDsU, sNsm, hyTc, GyJn, Kwvw, sPn...      1\n",
       "3  [vzCE, Ktvi, ynUk, Qrkv, sork, guFC, hfrR, DNK...      1\n",
       "4  [mdSU, jKhd, vzCE, ynUk, rlJO, pKEB, VegJ, pFZ...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "func = lambda x: x.split(' ')\n",
    "df.loc[:, 'sentence'] = df.loc[:, 'sentence'].apply(func)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: len(x)\n",
    "df.loc[:, 'count'] = df.loc[:, 'sentence'].apply(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1608108972.2371593\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-84e1153f272d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                  \u001b[0mmin_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                  \u001b[0mmax_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                  word_ngrams=1)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, sg, hs, size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, compatible_hash)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha)\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    815\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    551\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    486\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    487\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print(start)\n",
    "model = FastText(df[\"sentence\"], \n",
    "                 size=100,\n",
    "                 window=5, \n",
    "                 min_count=3,\n",
    "                 iter=1000, \n",
    "                 min_n=4,\n",
    "                 max_n=4,\n",
    "                 word_ngrams=1)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText.load(\"fasttext2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.54764426e+00, -1.22446299e-01, -2.60812664e+00, -4.95032644e+00,\n",
       "       -1.07825077e+00, -2.69971132e+00,  4.19377613e+00,  4.79241943e+00,\n",
       "        2.71984369e-01, -4.93719196e+00,  2.63291419e-01, -1.79461026e+00,\n",
       "       -1.13923728e+00, -1.31159222e+00,  1.15545504e-01, -3.26748991e+00,\n",
       "        3.06274652e+00,  6.45121396e-01, -1.60472107e+00, -3.40828872e+00,\n",
       "       -8.36644936e+00,  5.48713207e+00, -3.70728326e+00, -1.81285906e+00,\n",
       "       -6.11935997e+00, -3.40383172e+00,  8.44011188e-01,  2.81638718e+00,\n",
       "       -2.73316574e+00, -2.98882961e+00,  3.24945480e-01,  3.39753437e+00,\n",
       "        1.79928589e+00, -6.99303925e-01,  2.61142921e+00, -1.63246465e+00,\n",
       "       -1.35362387e+00,  2.69862747e+00, -4.78039789e+00, -9.97613335e+00,\n",
       "       -1.59017015e+00, -3.82022834e+00, -2.37984467e+00, -1.69990492e+00,\n",
       "        1.88297451e+00,  2.77985144e+00, -1.36188948e+00,  2.63869143e+00,\n",
       "        1.72610033e+00,  5.85422873e-01, -1.20852077e+00, -1.63478881e-01,\n",
       "        5.84770799e-01, -2.12104249e+00, -1.99499696e-01, -8.54643583e-02,\n",
       "        7.20453382e-01, -6.71297264e+00,  5.03328609e+00, -7.35110426e+00,\n",
       "        4.28637743e-01,  2.95800114e+00, -1.49210250e+00,  7.04018354e+00,\n",
       "        3.44053060e-01,  1.31871033e+00, -6.63050079e+00, -6.78537560e+00,\n",
       "        3.52011824e+00, -1.17380783e-01, -1.05768263e+00,  2.72093683e-01,\n",
       "        5.96157074e+00,  3.41441393e+00,  1.91258538e+00,  4.86421299e+00,\n",
       "       -4.27201796e+00,  5.94368577e-03, -4.29885054e+00,  3.92085028e+00,\n",
       "       -8.58694613e-01,  4.27920389e+00, -4.43464708e+00, -2.90580273e-01,\n",
       "        1.01417291e+00,  4.90375471e+00,  7.86584198e-01, -2.29660273e+00,\n",
       "        2.63577640e-01,  1.60259175e+00, -3.95218801e+00,  5.20272827e+00,\n",
       "        1.14666295e+00, -4.31026411e+00,  4.90033448e-01,  9.55655575e-01,\n",
       "       -1.93811104e-01,  1.09001207e+00,  8.36343575e+00,  6.44772482e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['rlJO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>value</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[rlJO, PERC, jKBF, vzCE, guFC, RCEb, RFai, sVy...</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[CXWx, YQuU, UKEG, VegJ, Mwlr, hDgE, rlJO, jRa...</td>\n",
       "      <td>1</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[rlJO, YcPU, VDsU, sNsm, hyTc, GyJn, Kwvw, sPn...</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[vzCE, Ktvi, ynUk, Qrkv, sork, guFC, hfrR, DNK...</td>\n",
       "      <td>1</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[mdSU, jKhd, vzCE, ynUk, rlJO, pKEB, VegJ, pFZ...</td>\n",
       "      <td>1</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  value  count\n",
       "0  [rlJO, PERC, jKBF, vzCE, guFC, RCEb, RFai, sVy...      0    169\n",
       "1  [CXWx, YQuU, UKEG, VegJ, Mwlr, hDgE, rlJO, jRa...      1    427\n",
       "2  [rlJO, YcPU, VDsU, sNsm, hyTc, GyJn, Kwvw, sPn...      1    179\n",
       "3  [vzCE, Ktvi, ynUk, Qrkv, sork, guFC, hfrR, DNK...      1    242\n",
       "4  [mdSU, jKhd, vzCE, ynUk, rlJO, pKEB, VegJ, pFZ...      1    472"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(df['count'])\n",
    "plt.savefig('count.png',dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rlJO',\n",
       " 'PERC',\n",
       " 'jKBF',\n",
       " 'vzCE',\n",
       " 'guFC',\n",
       " 'RCEb',\n",
       " 'RFai',\n",
       " 'sVyF',\n",
       " 'KuNh',\n",
       " 'putN',\n",
       " 'CHJl',\n",
       " 'sNsm',\n",
       " 'fcJH',\n",
       " 'fdqi',\n",
       " 'Ecjg',\n",
       " 'RsMg',\n",
       " 'YQuU',\n",
       " 'CXWx',\n",
       " 'rlJO',\n",
       " 'djGV',\n",
       " 'YQuU',\n",
       " 'rlJO',\n",
       " 'TvAs',\n",
       " 'jDHt',\n",
       " 'VLfQ',\n",
       " 'sNsm',\n",
       " 'KmgN',\n",
       " 'kQcg',\n",
       " 'eHpt',\n",
       " 'zUzf',\n",
       " 'VLfQ',\n",
       " 'sNsm',\n",
       " 'sssM',\n",
       " 'VLfQ',\n",
       " 'ETnB',\n",
       " 'sNsm',\n",
       " 'HtvU',\n",
       " 'MMUK',\n",
       " 'CXWx',\n",
       " 'KwKp',\n",
       " 'TvLX',\n",
       " 'ibGK',\n",
       " 'CXWx',\n",
       " 'MnHy',\n",
       " 'mdSU',\n",
       " 'IDHw',\n",
       " 'VLfQ',\n",
       " 'ePMg',\n",
       " 'UKEG',\n",
       " 'kQcg',\n",
       " 'rlJO',\n",
       " 'qJnw',\n",
       " 'ZxWD',\n",
       " 'lyhi',\n",
       " 'RYgh',\n",
       " 'Xsbz',\n",
       " 'jDdq',\n",
       " 'mdvU',\n",
       " 'mdSU',\n",
       " 'VSCW',\n",
       " 'oSKo',\n",
       " 'SyoQ',\n",
       " 'ETnB',\n",
       " 'tBEE',\n",
       " 'ttEg',\n",
       " 'fZYY',\n",
       " 'Qrkv',\n",
       " 'XBXv',\n",
       " 'Igxk',\n",
       " 'cOkB',\n",
       " 'VLfQ',\n",
       " 'NDlx',\n",
       " 'rlJO',\n",
       " 'PERC',\n",
       " 'ZDhf',\n",
       " 'vzCE',\n",
       " 'VyjM',\n",
       " 'sPnm',\n",
       " 'XBXv',\n",
       " 'OWXj',\n",
       " 'VLfQ',\n",
       " 'KYTM',\n",
       " 'CEIt',\n",
       " 'faSY',\n",
       " 'CXWx',\n",
       " 'rlJO',\n",
       " 'KPlf',\n",
       " 'YQuU',\n",
       " 'ghwR',\n",
       " 'KNod',\n",
       " 'sDrG',\n",
       " 'zgoU',\n",
       " 'sNsm',\n",
       " 'xfzW',\n",
       " 'FJYa',\n",
       " 'FCjt',\n",
       " 'sNsm',\n",
       " 'KmgN',\n",
       " 'kQcg',\n",
       " 'rlJO',\n",
       " 'ywqc',\n",
       " 'CfWK',\n",
       " 'ZxWD',\n",
       " 'sDrG',\n",
       " 'pKEB',\n",
       " 'pFZi',\n",
       " 'UbZf',\n",
       " 'ldar',\n",
       " 'vzCE',\n",
       " 'NOmJ',\n",
       " 'rlJO',\n",
       " 'FgdF',\n",
       " 'sDrG',\n",
       " 'IBQw',\n",
       " 'lfug',\n",
       " 'dQWP',\n",
       " 'Qrkv',\n",
       " 'XBXv',\n",
       " 'VZuF',\n",
       " 'NgCF',\n",
       " 'Cjvf',\n",
       " 'LcIO',\n",
       " 'lVjt',\n",
       " 'CXWx',\n",
       " 'qJnw',\n",
       " 'XBXv',\n",
       " 'uhik',\n",
       " 'LHXo',\n",
       " 'ynUk',\n",
       " 'VLfQ',\n",
       " 'vzCE',\n",
       " 'ynUk',\n",
       " 'CHJl',\n",
       " 'XBXv',\n",
       " 'VWPA',\n",
       " 'XOHN',\n",
       " 'ldar',\n",
       " 'XBXv',\n",
       " 'VyjM',\n",
       " 'sDrG',\n",
       " 'ywqc',\n",
       " 'CfWK',\n",
       " 'MVWT',\n",
       " 'CEIt',\n",
       " 'rLWK',\n",
       " 'TBav',\n",
       " 'VLfQ',\n",
       " 'KNod',\n",
       " 'dEMM',\n",
       " 'bALC',\n",
       " 'ScwJ',\n",
       " 'xpUB',\n",
       " 'lVHM',\n",
       " 'mdSU',\n",
       " 'guFC',\n",
       " 'dEMM',\n",
       " 'VPeh',\n",
       " 'IMnn',\n",
       " 'vzCE',\n",
       " 'ScwJ',\n",
       " 'fytm',\n",
       " 'YQuU',\n",
       " 'guFC',\n",
       " 'NOmJ',\n",
       " 'VLfQ',\n",
       " 'sNsm',\n",
       " 'RFai',\n",
       " 'VyjM',\n",
       " 'ALeE']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['sentence','value']]['sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train, train_length, y_train = [], [], []\n",
    "for line in range(22500):\n",
    "    content= df[['sentence','value']]['sentence'][line]\n",
    "    sentiment = df[['sentence','value']]['value'][line]\n",
    "    X, y = [], sentiment\n",
    "    for w in content[:max_length]:\n",
    "        if w in model:\n",
    "            X.append(np.expand_dims(model[w], 0))\n",
    "    if X:\n",
    "        length = len(X)\n",
    "        X = X + [np.zeros_like(X[0])] * (max_length - length)\n",
    "        X = np.concatenate(X)\n",
    "        X = np.expand_dims(X, 0)\n",
    "        X_train.append(X)\n",
    "        train_length.append(length)\n",
    "        y_train.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "X_test, test_length, y_test = [], [], []\n",
    "for line in range(2500):\n",
    "    line = line + 22500\n",
    "    content= df[['sentence','value']]['sentence'][line]\n",
    "    sentiment = df[['sentence','value']]['value'][line]\n",
    "    X, y = [], sentiment\n",
    "    for w in content[:max_length]:\n",
    "        if w in model:\n",
    "            X.append(np.expand_dims(model[w], 0))\n",
    "    if X:\n",
    "        length = len(X)\n",
    "        X = X + [np.zeros_like(X[0])] * (max_length - length)\n",
    "        X = np.concatenate(X)\n",
    "        X = np.expand_dims(X, 0)\n",
    "        X_test.append(X)\n",
    "        test_length.append(length)\n",
    "        y_test.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/hack01/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hack01/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hack01/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hack01/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hack01/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hack01/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn, seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-12-b5c5730db901>:15: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-12-b5c5730db901>:35: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f40a1f0a898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f40a1f0a898>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f40a1f0a898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f40a1f0a898>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From /home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f40a1f0a9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f40a1f0a9e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f40a1f0a9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f40a1f0a9e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f40a8de5860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f40a8de5860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f40a8de5860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f40a8de5860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f40a1f0ae48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f40a1f0ae48>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f40a1f0ae48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f40a1f0ae48>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f40a1eab710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f40a1eab710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f40a1eab710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f40a1eab710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f40a0eecba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f40a0eecba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f40a0eecba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f40a0eecba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn, seq2seq\n",
    "batch_size = 512\n",
    "lr = 1e-3\n",
    "hidden_size = 100\n",
    "\n",
    "X = tf.placeholder(shape=(batch_size, max_length, 100), dtype=tf.float32, name=\"X\")\n",
    "L = tf.placeholder(shape=(batch_size), dtype=np.int32, name=\"L\")\n",
    "y = tf.placeholder(shape=(batch_size, 1), dtype=np.float32, name=\"y\")\n",
    "dropout = tf.placeholder(shape=(), dtype=np.float32, name=\"dropout\")\n",
    "\n",
    "with tf.variable_scope(\"lstm\", reuse=tf.AUTO_REUSE):\n",
    "    def lstm_cell(hidden_size, cell_id=0):\n",
    "        # LSTM细胞生成器\n",
    "        cell = rnn.LSTMCell(hidden_size, reuse=tf.AUTO_REUSE, name='cell%d' % cell_id)\n",
    "        cell = rnn.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "        return cell\n",
    "    \n",
    "    context = tf.get_variable(\"context\", shape=(1, hidden_size))\n",
    "    context = tf.tile(context, [batch_size, 1])\n",
    "    \n",
    "    # BiLSTM部分\n",
    "    fw_cell = lstm_cell(hidden_size, 0)\n",
    "    bw_cell = lstm_cell(hidden_size, 1)\n",
    "    fw_zero = fw_cell.zero_state(batch_size, tf.float32)\n",
    "    bw_zero = fw_cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    # Seq2Seq版的dynamic_rnn\n",
    "    encoder_output, encoder_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,\n",
    "                                                         cell_bw=bw_cell,\n",
    "                                                         inputs=X,\n",
    "                                                         sequence_length=L,\n",
    "                                                         initial_state_fw=fw_zero,\n",
    "                                                         initial_state_bw=bw_zero,\n",
    "                                                         dtype=tf.float32)\n",
    "    \n",
    "    # Attention模块\n",
    "    attention_context = tf.concat(encoder_output, axis=2)\n",
    "    attention_mech = seq2seq.BahdanauAttention(hidden_size * 2,\n",
    "                                                 memory=attention_context,\n",
    "                                                 memory_sequence_length=L,\n",
    "                                                 name=\"AttentionMechanism\")\n",
    "    attention_cell = seq2seq.AttentionWrapper(cell=lstm_cell(hidden_size, 2),\n",
    "                                                attention_mechanism=attention_mech,\n",
    "                                                attention_layer_size=hidden_size,\n",
    "                                                alignment_history=True,\n",
    "                                                output_attention=True,\n",
    "                                                name=\"AttentionCell\")\n",
    "    \n",
    "    # Attention加权得到的context向量\n",
    "    attention_zero = attention_cell.zero_state(batch_size, tf.float32)\n",
    "    attention_output, attention_state = attention_cell.call(context, attention_zero)\n",
    "    aligments = attention_state[3]\n",
    "    \n",
    "    # 用context向量直接用MLP做二分类\n",
    "    W1 = tf.get_variable(\"W1\", shape=(hidden_size, 50))\n",
    "    b1 = tf.get_variable(\"b1\", shape=(50,))\n",
    "    W2 = tf.get_variable(\"W2\", shape=(50, 1))\n",
    "    b2 = tf.get_variable(\"b2\", shape=(1,))\n",
    "    fcn1 = tf.nn.xw_plus_b(attention_output, W1, b1)\n",
    "    logists = tf.nn.xw_plus_b(fcn1, W2, b2)\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logists, labels=y))     # 交叉熵\n",
    "    op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f11a6da8ac8>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3363, in run_code\n",
      "    return outflag  File \"<ipython-input-29-29d693d604eb>\", line 63, in <module>\n",
      "    op = tf.train.AdamOptimizer(lr).minimize(loss)  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 2534, in call\n",
      "    return attention, next_state  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1191, in write\n",
      "    return self._implementation.write(index, value, name=name)  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs))\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f12778743c8>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3363, in run_code\n",
      "    return outflag  File \"<ipython-input-30-29d693d604eb>\", line 63, in <module>\n",
      "    op = tf.train.AdamOptimizer(lr).minimize(loss)  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 2534, in call\n",
      "    return attention, next_state  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1191, in write\n",
      "    return self._implementation.write(index, value, name=name)  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs))\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f10f2308f98>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3363, in run_code\n",
      "    return outflag  File \"<ipython-input-31-29d693d604eb>\", line 63, in <module>\n",
      "    op = tf.train.AdamOptimizer(lr).minimize(loss)  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 2534, in call\n",
      "    return attention, next_state  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1191, in write\n",
      "    return self._implementation.write(index, value, name=name)  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs))\n",
      "==================================\n",
      "INFO:tensorflow:Restoring parameters from model3/model-4460\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.import_meta_graph(\"model3/model-4460.meta\")\n",
    "saver.restore(sess, \"model3/model-4460\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = 10000\n",
    "step = 0\n",
    "cursor = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0  loss: 0.66971004\n",
      "step: 10  loss: 0.48834503\n",
      "WARNING:tensorflow:From /home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "step: 20  loss: 0.3065982\n",
      "step: 30  loss: 0.3012137\n",
      "step: 40  loss: 0.24097091\n",
      "step: 50  loss: 0.28552607\n",
      "step: 60  loss: 0.22231142\n",
      "step: 70  loss: 0.25020492\n",
      "step: 80  loss: 0.2648315\n",
      "step: 90  loss: 0.2687641\n",
      "step: 100  loss: 0.22271079\n",
      "step: 110  loss: 0.259233\n",
      "step: 120  loss: 0.21537942\n",
      "step: 130  loss: 0.22913834\n",
      "step: 140  loss: 0.19778997\n",
      "step: 150  loss: 0.17544849\n",
      "step: 160  loss: 0.20455775\n",
      "step: 170  loss: 0.17193738\n",
      "step: 180  loss: 0.19399631\n",
      "step: 190  loss: 0.15867564\n",
      "step: 200  loss: 0.137312\n",
      "step: 210  loss: 0.18830387\n",
      "step: 220  loss: 0.1221168\n",
      "step: 230  loss: 0.16068004\n",
      "step: 240  loss: 0.11718157\n",
      "step: 250  loss: 0.1406255\n",
      "step: 260  loss: 0.097898364\n",
      "step: 270  loss: 0.14776024\n",
      "step: 280  loss: 0.09407307\n",
      "step: 290  loss: 0.1033435\n",
      "step: 300  loss: 0.12245402\n",
      "step: 310  loss: 0.1276871\n",
      "step: 320  loss: 0.11535512\n",
      "step: 330  loss: 0.113293365\n",
      "step: 340  loss: 0.08847695\n",
      "step: 350  loss: 0.08946101\n",
      "step: 360  loss: 0.09751457\n",
      "step: 370  loss: 0.07433267\n",
      "step: 380  loss: 0.09151694\n",
      "step: 390  loss: 0.04252893\n",
      "step: 400  loss: 0.12815207\n",
      "step: 410  loss: 0.072392136\n",
      "step: 420  loss: 0.055187147\n",
      "step: 430  loss: 0.05674001\n",
      "step: 440  loss: 0.050525777\n",
      "step: 450  loss: 0.06473935\n",
      "step: 460  loss: 0.121440165\n",
      "step: 470  loss: 0.057460368\n",
      "step: 480  loss: 0.0708247\n",
      "step: 510  loss: 0.05098451\n",
      "step: 520  loss: 0.05916485\n",
      "step: 530  loss: 0.07456912\n",
      "step: 540  loss: 0.06526023\n",
      "step: 560  loss: 0.026806608\n",
      "step: 570  loss: 0.060461815\n",
      "step: 580  loss: 0.11309412\n",
      "step: 590  loss: 0.09868334\n",
      "step: 600  loss: 0.07824804\n",
      "step: 610  loss: 0.039980315\n",
      "step: 620  loss: 0.062144905\n",
      "step: 630  loss: 0.12026462\n",
      "step: 640  loss: 0.11321198\n",
      "step: 650  loss: 0.048886705\n",
      "step: 660  loss: 0.033198513\n",
      "step: 670  loss: 0.09580548\n",
      "step: 680  loss: 0.039306283\n",
      "step: 690  loss: 0.056815833\n",
      "step: 700  loss: 0.09096421\n",
      "step: 710  loss: 0.0785383\n",
      "step: 720  loss: 0.018192433\n",
      "step: 730  loss: 0.015528193\n",
      "step: 740  loss: 0.0171814\n",
      "step: 750  loss: 0.015331981\n",
      "step: 760  loss: 0.009682017\n",
      "step: 770  loss: 0.012338573\n",
      "step: 780  loss: 0.037049077\n",
      "step: 800  loss: 0.021243693\n",
      "step: 810  loss: 0.03271114\n",
      "step: 820  loss: 0.020194473\n",
      "step: 830  loss: 0.018578852\n",
      "step: 840  loss: 0.009857206\n",
      "step: 850  loss: 0.020441804\n",
      "step: 860  loss: 0.012414435\n",
      "step: 870  loss: 0.0054541696\n",
      "step: 880  loss: 0.017749587\n",
      "step: 890  loss: 0.033669904\n",
      "step: 900  loss: 0.015876131\n",
      "step: 910  loss: 0.0053794403\n",
      "step: 920  loss: 0.03145746\n",
      "step: 930  loss: 0.014390379\n",
      "step: 940  loss: 0.02242859\n",
      "step: 950  loss: 0.014063618\n",
      "step: 960  loss: 0.025100013\n",
      "step: 970  loss: 0.031809624\n",
      "step: 980  loss: 0.013712293\n",
      "step: 990  loss: 0.02619604\n",
      "step: 1000  loss: 0.022810753\n",
      "step: 1010  loss: 0.022336371\n",
      "step: 1020  loss: 0.0141593255\n",
      "step: 1030  loss: 0.0805148\n",
      "step: 1040  loss: 0.0031962874\n",
      "step: 1050  loss: 0.010020297\n",
      "step: 1060  loss: 0.07831414\n",
      "step: 1070  loss: 0.007353845\n",
      "step: 1080  loss: 0.011421109\n",
      "step: 1090  loss: 0.084414065\n",
      "step: 1100  loss: 0.019198135\n",
      "step: 1110  loss: 0.012965642\n",
      "step: 1120  loss: 0.042372085\n",
      "step: 1130  loss: 0.11854923\n",
      "step: 1140  loss: 0.015980292\n",
      "step: 1150  loss: 0.052196234\n",
      "step: 1160  loss: 0.08527006\n",
      "step: 1170  loss: 0.04506912\n",
      "step: 1180  loss: 0.008718866\n",
      "step: 1190  loss: 0.010563081\n",
      "step: 1200  loss: 0.015904488\n",
      "step: 1210  loss: 0.005505532\n",
      "step: 1220  loss: 0.0045111044\n",
      "step: 1230  loss: 0.0018518146\n",
      "step: 1240  loss: 0.0023490475\n",
      "step: 1250  loss: 0.029899957\n",
      "step: 1260  loss: 0.0011895357\n",
      "step: 1270  loss: 0.003551989\n",
      "step: 1280  loss: 0.0005774677\n",
      "step: 1290  loss: 0.00061172317\n",
      "step: 1300  loss: 0.0006040085\n",
      "step: 1310  loss: 0.0009955018\n",
      "step: 1320  loss: 0.010223495\n",
      "step: 1330  loss: 0.0002772242\n",
      "step: 1340  loss: 0.000408264\n",
      "step: 1350  loss: 0.00050529087\n",
      "step: 1360  loss: 0.009646906\n",
      "step: 1370  loss: 0.0026871804\n",
      "step: 1380  loss: 0.0013305338\n",
      "step: 1390  loss: 0.0020131038\n",
      "step: 1400  loss: 0.0016701851\n",
      "step: 1410  loss: 0.0015162724\n",
      "step: 1420  loss: 0.0005670204\n",
      "step: 1430  loss: 0.0002849624\n",
      "step: 1440  loss: 0.0002679542\n",
      "step: 1450  loss: 0.0003990049\n",
      "step: 1460  loss: 0.00065273704\n",
      "step: 1470  loss: 0.0001143564\n",
      "step: 1480  loss: 0.00013350358\n",
      "step: 1490  loss: 0.0018550724\n",
      "step: 1500  loss: 0.00025900616\n",
      "step: 1510  loss: 0.00017160366\n",
      "step: 1520  loss: 0.00033179973\n",
      "step: 1530  loss: 0.0004922746\n",
      "step: 1540  loss: 0.00058194145\n",
      "step: 1550  loss: 0.0003463029\n",
      "step: 1560  loss: 0.00056681124\n",
      "step: 1570  loss: 0.00018027815\n",
      "step: 1580  loss: 0.0001471465\n",
      "step: 1590  loss: 0.0002583758\n",
      "step: 1600  loss: 7.6934775e-05\n",
      "step: 1610  loss: 0.000115007526\n",
      "step: 1620  loss: 0.00010411484\n",
      "step: 1630  loss: 5.8588706e-05\n",
      "step: 1640  loss: 0.00014463314\n",
      "step: 1650  loss: 6.39412e-05\n",
      "step: 1660  loss: 8.101842e-05\n",
      "step: 1670  loss: 0.00061002927\n",
      "step: 1680  loss: 0.00013304058\n",
      "step: 1690  loss: 0.00015844587\n",
      "step: 1700  loss: 0.00022914063\n",
      "step: 1710  loss: 7.630164e-05\n",
      "step: 1720  loss: 0.000478789\n",
      "step: 1730  loss: 5.663277e-05\n",
      "step: 1740  loss: 3.799131e-05\n",
      "step: 1750  loss: 0.00017500184\n",
      "step: 1760  loss: 2.441039e-05\n",
      "step: 1770  loss: 0.006336881\n",
      "step: 1780  loss: 0.00012506907\n",
      "step: 1790  loss: 0.0004454378\n",
      "step: 1800  loss: 0.00048277908\n",
      "step: 1810  loss: 6.7415196e-05\n",
      "step: 1820  loss: 0.00014507401\n",
      "step: 1830  loss: 0.00028929015\n",
      "step: 1840  loss: 0.0010587034\n",
      "step: 1850  loss: 0.00047760416\n",
      "step: 1860  loss: 0.0012883961\n",
      "step: 1870  loss: 9.7040785e-05\n",
      "step: 1880  loss: 5.485725e-05\n",
      "step: 1890  loss: 0.00016785374\n",
      "step: 1900  loss: 3.345429e-05\n",
      "step: 1910  loss: 0.00024182632\n",
      "step: 1920  loss: 0.0038205876\n",
      "step: 1930  loss: 0.00033911964\n",
      "step: 1940  loss: 0.00060557254\n",
      "step: 1950  loss: 0.00034843618\n",
      "step: 1960  loss: 0.00050771906\n",
      "step: 1970  loss: 0.010379153\n",
      "step: 1980  loss: 0.0011036308\n",
      "step: 1990  loss: 0.003380141\n",
      "step: 2000  loss: 0.000817765\n",
      "step: 2010  loss: 0.0006047153\n",
      "step: 2020  loss: 0.00021706457\n",
      "step: 2030  loss: 9.850669e-05\n"
     ]
    }
   ],
   "source": [
    "while step < total_step:\n",
    "    _X, _L, _y = X_train[cursor: cursor + batch_size], train_length[cursor: cursor + batch_size], y_train[cursor: cursor + batch_size]\n",
    "    cursor += batch_size\n",
    "    if len(_X) < batch_size:\n",
    "        cursor = batch_size - len(_X)\n",
    "        _X += X_train[: cursor]\n",
    "        _L += train_length[: cursor]\n",
    "        _y += y_train[: cursor]\n",
    "    _X = np.concatenate(_X)\n",
    "    _L = np.reshape(np.array(_L, dtype=np.int32), (-1))\n",
    "    _y = np.reshape(np.array(_y, dtype=np.float32), (batch_size, 1))\n",
    "    _, l = sess.run([op, loss], feed_dict={X: _X, L:_L, y: _y, dropout:.5})\n",
    "    if step % 10 == 0:\n",
    "        print(\"step:\", step, \" loss:\", l)\n",
    "        saver.save(sess,'model5/model', global_step=step)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4000  loss: 0.0014862489\n",
      "step: 4010  loss: 9.20491e-05\n",
      "step: 4020  loss: 0.0022914985\n",
      "step: 4030  loss: 0.00042103598\n",
      "step: 4040  loss: 0.004100859\n",
      "step: 4050  loss: 0.00019080049\n",
      "step: 4060  loss: 0.00048707475\n",
      "step: 4070  loss: 0.010903294\n",
      "step: 4080  loss: 0.007448681\n",
      "step: 4090  loss: 0.0009627164\n",
      "step: 4100  loss: 0.0029313369\n",
      "step: 4110  loss: 0.000512026\n",
      "step: 4120  loss: 0.0024424188\n",
      "step: 4130  loss: 0.007675872\n",
      "step: 4140  loss: 0.00047229003\n",
      "step: 4150  loss: 0.00541658\n",
      "step: 4160  loss: 0.00074621494\n",
      "step: 4170  loss: 0.00020351824\n",
      "step: 4180  loss: 0.0017561887\n",
      "step: 4190  loss: 0.00029589786\n",
      "step: 4200  loss: 0.0006316444\n",
      "step: 4210  loss: 0.00043740694\n",
      "step: 4220  loss: 0.0032544401\n",
      "step: 4230  loss: 0.00013980185\n",
      "step: 4240  loss: 0.0042976923\n",
      "step: 4250  loss: 0.002145079\n",
      "step: 4260  loss: 0.0027688676\n",
      "step: 4270  loss: 0.00040232774\n",
      "step: 4280  loss: 0.00033490168\n",
      "step: 4290  loss: 0.003608546\n",
      "step: 4300  loss: 0.00044068764\n",
      "step: 4310  loss: 0.0008093553\n",
      "step: 4320  loss: 0.00039359689\n",
      "step: 4330  loss: 0.00013086527\n",
      "step: 4340  loss: 0.006948781\n",
      "step: 4350  loss: 0.00074808905\n",
      "step: 4360  loss: 0.015762316\n",
      "step: 4370  loss: 0.004925639\n",
      "step: 4380  loss: 0.00026694092\n",
      "step: 4390  loss: 0.008418781\n",
      "step: 4400  loss: 0.0024199213\n",
      "step: 4410  loss: 0.0032361206\n",
      "step: 4420  loss: 0.0001246213\n",
      "step: 4430  loss: 0.014073179\n",
      "step: 4440  loss: 0.0048494292\n",
      "step: 4450  loss: 0.0018642277\n",
      "step: 4460  loss: 0.018188681\n"
     ]
    }
   ],
   "source": [
    "while step < total_step:\n",
    "    _X, _L, _y = X_train[cursor: cursor + batch_size], train_length[cursor: cursor + batch_size], y_train[cursor: cursor + batch_size]\n",
    "    cursor += batch_size\n",
    "    if len(_X) < batch_size:\n",
    "        cursor = batch_size - len(_X)\n",
    "        _X += X_train[: cursor]\n",
    "        _L += train_length[: cursor]\n",
    "        _y += y_train[: cursor]\n",
    "    _X = np.concatenate(_X)\n",
    "    _L = np.reshape(np.array(_L, dtype=np.int32), (-1))\n",
    "    _y = np.reshape(np.array(_y, dtype=np.float32), (batch_size, 1))\n",
    "    _, l = sess.run([op, loss], feed_dict={X: _X, L:_L, y: _y, dropout:.5})\n",
    "    if step % 10 == 0:\n",
    "        print(\"step:\", step, \" loss:\", l)\n",
    "        saver.save(sess,'model3/model', global_step=step)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sum = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "prediction = []\n",
    "while step < test_sum:\n",
    "    if step + batch_size <= test_sum:\n",
    "        result = sess.run(tf.nn.sigmoid(logists), feed_dict={X: np.concatenate(X_test[step:step+512]), L: np.array(test_length[step:step+512]), dropout:1.})\n",
    "        for i in result:\n",
    "            if i > 0.5:\n",
    "                prediction.append(1)\n",
    "            else:\n",
    "                prediction.append(0)\n",
    "    else:\n",
    "        _X = np.concatenate(X_test[step:] + [np.zeros_like(X_test[0])] * (batch_size - len(X_test[step:])))\n",
    "        _L = np.array(test_length[step:] + [1] * (batch_size - len(test_length[step:])))\n",
    "        result = sess.run(tf.nn.sigmoid(logists), feed_dict={X: _X, L: _L, dropout:1.})\n",
    "        for i in result[:len(test_length[step:])]:\n",
    "            if i > 0.5:\n",
    "                prediction.append(1)\n",
    "            else:\n",
    "                prediction.append(0)\n",
    "    step += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      2504\n",
      "           1       0.88      0.90      0.89      2496\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n",
      "准确率: 0.8934\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(\"准确率:\", metrics.accuracy_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.88      2504\n",
      "           1       0.86      0.92      0.89      2496\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n",
      "准确率: 0.8886\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(\"准确率:\", metrics.accuracy_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      2504\n",
      "           1       0.91      0.87      0.89      2496\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n",
      "准确率: 0.8918\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(\"准确率:\", metrics.accuracy_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      1259\n",
      "           1       0.89      0.91      0.90      1241\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "准确率: 0.9008\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(\"准确率:\", metrics.accuracy_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90      1259\n",
      "           1       0.90      0.90      0.90      1241\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "准确率: 0.9012\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(\"准确率:\", metrics.accuracy_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      1259\n",
      "           1       0.89      0.91      0.90      1241\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "准确率: 0.8988\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(\"准确率:\", metrics.accuracy_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90      1259\n",
      "           1       0.90      0.90      0.90      1241\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "准确率: 0.8964\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(\"准确率:\", metrics.accuracy_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model3/model-4460\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('model3/model-4460.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./model3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sum = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fab3a7a19e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fab3a7a19e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fab3a7a19e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fab3a7a19e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fab39e14f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fab39e14f98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fab39e14f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fab39e14f98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab391c0828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab391c0828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab391c0828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab391c0828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fac67d93668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fac67d93668>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fac67d93668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fac67d93668>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab39ba70f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab39ba70f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab39ba70f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab39ba70f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab20b5e780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab20b5e780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab20b5e780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fab20b5e780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fac6795d2b0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3363, in run_code\n",
      "    return outflag  File \"<ipython-input-16-d4f4b4d9d8aa>\", line 62, in <module>\n",
      "    op = tf.train.AdamOptimizer(lr).minimize(loss)  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 2534, in call\n",
      "    return attention, next_state  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1191, in write\n",
      "    return self._implementation.write(index, value, name=name)  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs))\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fab22c73eb8>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3363, in run_code\n",
      "    return outflag  File \"<ipython-input-17-73939d1d6074>\", line 62, in <module>\n",
      "    op = tf.train.AdamOptimizer(lr).minimize(loss)  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 2534, in call\n",
      "    return attention, next_state  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1191, in write\n",
      "    return self._implementation.write(index, value, name=name)  File \"/home/hack01/.local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs))\n",
      "==================================\n",
      "INFO:tensorflow:Restoring parameters from ./model3/model-4460\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 512\n",
    "lr = 1e-3\n",
    "hidden_size = 100\n",
    "\n",
    "X = tf.placeholder(shape=(batch_size, max_length, 100), dtype=tf.float32, name=\"X\")\n",
    "L = tf.placeholder(shape=(batch_size), dtype=np.int32, name=\"L\")\n",
    "y = tf.placeholder(shape=(batch_size, 1), dtype=np.float32, name=\"y\")\n",
    "dropout = tf.placeholder(shape=(), dtype=np.float32, name=\"dropout\")\n",
    " \n",
    "with tf.variable_scope(\"lstm\", reuse=tf.AUTO_REUSE):\n",
    "    \n",
    "    def lstm_cell(hidden_size, cell_id=0):\n",
    "        # LSTM细胞生成器\n",
    "        cell = rnn.LSTMCell(hidden_size, reuse=tf.AUTO_REUSE, name='cell%d' % cell_id)\n",
    "        cell = rnn.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "        return cell\n",
    "    \n",
    "    context = tf.get_variable(\"context\", shape=(1, hidden_size))\n",
    "    context = tf.tile(context, [batch_size, 1])\n",
    "    \n",
    "    # BiLSTM部分\n",
    "    fw_cell = lstm_cell(hidden_size, 0)\n",
    "    bw_cell = lstm_cell(hidden_size, 1)\n",
    "    fw_zero = fw_cell.zero_state(batch_size, tf.float32)\n",
    "    bw_zero = fw_cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    # Seq2Seq版的dynamic_rnn\n",
    "    encoder_output, encoder_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,\n",
    "                                                         cell_bw=bw_cell,\n",
    "                                                         inputs=X,\n",
    "                                                         sequence_length=L,\n",
    "                                                         initial_state_fw=fw_zero,\n",
    "                                                         initial_state_bw=bw_zero,\n",
    "                                                         dtype=tf.float32)\n",
    "    \n",
    "    # Attention模块\n",
    "    attention_context = tf.concat(encoder_output, axis=2)\n",
    "    attention_mech = seq2seq.BahdanauAttention(hidden_size * 2,\n",
    "                                                 memory=attention_context,\n",
    "                                                 memory_sequence_length=L,\n",
    "                                                 name=\"AttentionMechanism\")\n",
    "    attention_cell = seq2seq.AttentionWrapper(cell=lstm_cell(hidden_size, 2),\n",
    "                                                attention_mechanism=attention_mech,\n",
    "                                                attention_layer_size=hidden_size,\n",
    "                                                alignment_history=True,\n",
    "                                                output_attention=True,\n",
    "                                                name=\"AttentionCell\")\n",
    "    \n",
    "    # Attention加权得到的context向量\n",
    "    attention_zero = attention_cell.zero_state(batch_size, tf.float32)\n",
    "    attention_output, attention_state = attention_cell.call(context, attention_zero)\n",
    "    aligments = attention_state[3]\n",
    "    \n",
    "    # 用context向量直接用MLP做二分类\n",
    "    W1 = tf.get_variable(\"W1\", shape=(hidden_size, 50))\n",
    "    b1 = tf.get_variable(\"b1\", shape=(50,))\n",
    "    W2 = tf.get_variable(\"W2\", shape=(50, 1))\n",
    "    b2 = tf.get_variable(\"b2\", shape=(1,))\n",
    "    fcn1 = tf.nn.xw_plus_b(attention_output, W1, b1)\n",
    "    logists = tf.nn.xw_plus_b(fcn1, W2, b2)\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logists, labels=y))     # 交叉熵\n",
    "    op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver() \n",
    "saver.restore(sess, tf.train.latest_checkpoint('./model3'))\n",
    "    \n",
    "step = 0\n",
    "prediction = []\n",
    "while step < test_sum:\n",
    "    if step + batch_size <= test_sum:\n",
    "        result = sess.run(tf.nn.sigmoid(logists), feed_dict={X: np.concatenate(X_test[step:step+batch_size]), L: np.array(test_length[step:step+batch_size]), dropout:1.})\n",
    "        for i in result:\n",
    "            if i > 0.5:\n",
    "                prediction.append(1)\n",
    "            else:\n",
    "                prediction.append(0)\n",
    "    else:\n",
    "        _X = np.concatenate(X_test[step:] + [np.zeros_like(X_test[0])] * (batch_size - len(X_test[step:])))\n",
    "        _L = np.array(test_length[step:] + [1] * (batch_size - len(test_length[step:])))\n",
    "        result = sess.run(tf.nn.sigmoid(logists), feed_dict={X: _X, L: _L, dropout:1.})\n",
    "        for i in result[:len(test_length[step:])]:\n",
    "            if i > 0.5:\n",
    "                 prediction.append(1)\n",
    "            else:\n",
    "                prediction.append(0)\n",
    "    step += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90      1259\n",
      "           1       0.90      0.91      0.90      1241\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "准确率: 0.9048\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(\"准确率:\", metrics.accuracy_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      1259\n",
      "           1       0.89      0.91      0.90      1241\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "准确率: 0.9008\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(\"准确率:\", metrics.accuracy_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88      1259\n",
      "           1       0.84      0.94      0.89      1241\n",
      "\n",
      "    accuracy                           0.88      2500\n",
      "   macro avg       0.89      0.89      0.88      2500\n",
      "weighted avg       0.89      0.88      0.88      2500\n",
      "\n",
      "准确率: 0.8848\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(\"准确率:\", metrics.accuracy_score(y_test, prediction))\n",
    "print(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90      1259\n",
      "           1       0.90      0.90      0.90      1241\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "准确率: 0.9016\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(\"准确率:\", metrics.accuracy_score(y_test, prediction))\n",
    "print(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90      1259\n",
      "           1       0.91      0.89      0.90      1241\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "准确率: 0.8972\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(\"准确率:\", metrics.accuracy_score(y_test, prediction))\n",
    "print(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.75      0.58      1259\n",
      "           1       0.39      0.16      0.23      1241\n",
      "\n",
      "    accuracy                           0.46      2500\n",
      "   macro avg       0.43      0.46      0.41      2500\n",
      "weighted avg       0.43      0.46      0.41      2500\n",
      "\n",
      "准确率: 0.4596\n",
      "4460\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(\"准确率:\", metrics.accuracy_score(y_test, prediction))\n",
    "print(4460)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model3/model-4460\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph(\"model3/model-4460.meta\")\n",
    "    new_saver.restore(sess, 'model3/model-4460')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "prediction = []\n",
    "while step < test_sum:\n",
    "    if step + batch_size <= test_sum:\n",
    "        result = sess.run(tf.nn.sigmoid(logists), feed_dict={X: np.concatenate(X_test[step:step+512]), L: np.array(test_length[step:step+512]), dropout:1.})\n",
    "        for i in result:\n",
    "            if i > 0.5:\n",
    "                prediction.append(1)\n",
    "            else:\n",
    "                prediction.append(0)\n",
    "    else:\n",
    "        _X = np.concatenate(X_test[step:] + [np.zeros_like(X_test[0])] * (batch_size - len(X_test[step:])))\n",
    "        _L = np.array(test_length[step:] + [1] * (batch_size - len(test_length[step:])))\n",
    "        result = sess.run(tf.nn.sigmoid(logists), feed_dict={X: _X, L: _L, dropout:1.})\n",
    "        for i in result[:len(test_length[step:])]:\n",
    "            if i > 0.5:\n",
    "                prediction.append(1)\n",
    "            else:\n",
    "                prediction.append(0)\n",
    "    step += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
